{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649d39a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3770127683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculate_psnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_ssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.train_data_functions import TrainData\n",
    "from utils.val_data_functions import ValData\n",
    "from utils.metrics import calculate_psnr, calculate_ssim\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.utils as tvu\n",
    "import cv2\n",
    "from model.cmformer import CMFormer\n",
    "\n",
    "from utils.losses import VGGPerceptualLoss, GradientLoss\n",
    "\n",
    "\n",
    "# --- Parse hyper-parameters  --- #\n",
    "parser = argparse.ArgumentParser(description='Hyper-parameters for network')\n",
    "parser.add_argument('-learning_rate', help='Set the learning rate', default=2e-4, type=float)\n",
    "parser.add_argument('-crop_size', help='Set the crop_size', default=[128, 128], nargs='+', type=int)\n",
    "parser.add_argument('-train_batch_size', help='Set the training batch size', default=18, type=int)\n",
    "parser.add_argument('-epoch_start', help='Starting epoch number of the training', default=0, type=int)\n",
    "#parser.add_argument('-lambda_loss', help='Set the lambda in loss function', default=0.04, type=float)\n",
    "parser.add_argument('-val_batch_size', help='Set the validation/test batch size', default=1, type=int)\n",
    "parser.add_argument('-exp_name', help='directory for saving the networks of the experiment', type=str)\n",
    "parser.add_argument('-seed', help='set random seed', default=19, type=int)\n",
    "parser.add_argument('-num_epochs', help='number of epochs', default=200, type=int)\n",
    "parser.add_argument('-num_steps', help='number of epochs', default=90000, type=int)\n",
    "parser.add_argument('-checkpoint', help='resume checkpoint', type=str)\n",
    "parser.add_argument('-save_epoch', help='save per epoch', default=10, type=int)\n",
    "parser.add_argument('-save_step', help='save per step', default=1000, type=int)\n",
    "parser.add_argument('-train_data_dir', help='train dataset path', type=str)\n",
    "parser.add_argument('-val_data_dir', help='test dataset path', type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "num_steps = args.num_steps\n",
    "save_step = args.save_step\n",
    "learning_rate = args.learning_rate\n",
    "crop_size = args.crop_size\n",
    "train_batch_size = args.train_batch_size\n",
    "epoch_start = args.epoch_start\n",
    "val_batch_size = args.val_batch_size\n",
    "exp_name = args.exp_name\n",
    "num_epochs = args.num_epochs\n",
    "save_epoch = args.save_epoch\n",
    "train_data_dir = args.train_data_dir\n",
    "val_data_dir = args.val_data_dir\n",
    "\n",
    "#create directory to save checkpoints\n",
    "if not os.path.exists(exp_name):\n",
    "    os.makedirs(exp_name, exist_ok=True)\n",
    "\n",
    "def save_image(img, file_directory):\n",
    "    if not os.path.exists(os.path.dirname(file_directory)):\n",
    "        os.makedirs(os.path.dirname(file_directory))\n",
    "    tvu.save_image(img, file_directory)\n",
    "\n",
    "# set seed\n",
    "seed = args.seed\n",
    "if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "print('--- Hyper-parameters for training ---')\n",
    "print(\n",
    "    'learning_rate: {}\\ncrop_size: {}\\ntrain_batch_size: {}\\nval_batch_size: {}\\n'.format(learning_rate,\n",
    "                                                                                                         crop_size,\n",
    "                                                                                                         train_batch_size,\n",
    "                                                                                                         val_batch_size,))\n",
    "print(f'training dataset path:{train_data_dir}, testing dataset path:{val_data_dir}')\n",
    "\n",
    "\n",
    "# --- Gpu device --- #\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Define the network --- #\n",
    "net = CMFormer()\n",
    "total = sum([param.nelement() for param in net.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total / 1e6))\n",
    "\n",
    "# --- Build optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Multi-GPU --- #\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "pixel_loss_fn = nn.L1Loss()\n",
    "perceptual_loss_fn = VGGPerceptualLoss().to(device)\n",
    "gradient_loss_fn = GradientLoss().to(device)\n",
    "\n",
    "\n",
    "# --- Load the network weight --- #\n",
    "chk = args.checkpoint\n",
    "if chk is not None:\n",
    "    try:\n",
    "        net.load_state_dict(torch.load(chk))\n",
    "        print('--- weight loaded ---')\n",
    "    except:\n",
    "        raise FileNotFoundError(f\"The file at path '{chk}' does not exist.\")\n",
    "\n",
    "\n",
    "# --- Load training data and validation/test data --- #\n",
    "\n",
    "labeled_name = 'train.txt'\n",
    "\n",
    "\n",
    "val_filename1 = 'test.txt'\n",
    "\n",
    "# --- Load training data and validation/test data --- #\n",
    "lbl_train_data_loader = DataLoader(TrainData(crop_size, train_data_dir, labeled_name, random_flip=True, random_rotate=True), batch_size=train_batch_size,\n",
    "                                   shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# val_data_loader = DataLoader(ValData(val_data_dir,val_filename), batch_size=val_batch_size, shuffle=False, num_workers=8)\n",
    "val_data_loader1 = DataLoader(ValData(val_data_dir, val_filename1), batch_size=val_batch_size, shuffle=False,\n",
    "                              num_workers=0)\n",
    "\n",
    "def denormalize(x):\n",
    "    return (x + 1) / 2\n",
    "\n",
    "def hub_loss(img, gt):\n",
    "    c = 0.03\n",
    "    diff = torch.sqrt(torch.pow(img - gt, 2) + c ** 2)\n",
    "    loss = diff - c\n",
    "    loss = loss.sum() / loss.numel()\n",
    "    return loss\n",
    "\n",
    "file_path = \"./eva/eva.txt\"\n",
    "total_steps = 0\n",
    "net.train()\n",
    "if chk:\n",
    "    total_steps = int(chk.split(\"/\")[-1].split(\"_\")[0])\n",
    "while True:\n",
    "    for batch_id, train_data in enumerate(lbl_train_data_loader):\n",
    "\n",
    "        input_image, gt, imgid = train_data\n",
    "        input_image = input_image.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # --- Zero the parameter gradients --- #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward + Backward + Optimize --- #\n",
    "        net.train()\n",
    "        pred_image = net(input_image)\n",
    "\n",
    "        # loss = hub_loss(pred_image, gt)\n",
    "        l_pixel = pixel_loss_fn(pred_image, gt)\n",
    "        l_perc = perceptual_loss_fn(\n",
    "            denormalize(pred_image),\n",
    "            denormalize(gt))\n",
    "        l_grad = gradient_loss_fn(pred_image, gt)\n",
    "\n",
    "        loss = (\n",
    "            1.0 * l_pixel +\n",
    "            0.1 * l_perc +\n",
    "            0.05 * l_grad\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_steps += 1\n",
    "\n",
    "        if (total_steps % 10) == 0:\n",
    "            # print('Steps: {0}, loss: {1}'.format(total_steps, loss))\n",
    "            print(f\"Steps:{total_steps} | \"\n",
    "                    f\"L1:{l_pixel:.4f} | \"\n",
    "                    f\"Perc:{l_perc:.4f} | \"\n",
    "                    f\"Grad:{l_grad:.4f}\")\n",
    "\n",
    "        if (total_steps % 100) == 0:\n",
    "            with torch.no_grad():\n",
    "                save_image(pred_image, os.path.join(\"./train_res\", f\"output.png\"))\n",
    "                save_image(gt, os.path.join(\"./train_res\", f\"gt.png\"))\n",
    "                save_image(input_image, os.path.join(\"./train_res\", f\"input.png\"))\n",
    "        # --- Test --- #\n",
    "        if total_steps % save_step == 0:\n",
    "            net.eval()\n",
    "            i = 0\n",
    "            with torch.no_grad():\n",
    "                start_time = time.time()\n",
    "                for batch_id, test_data in enumerate(val_data_loader1):\n",
    "                    input_image, gt, imgid = test_data\n",
    "                    input_image = input_image.to(device)\n",
    "                    gt = gt.to(device)\n",
    "                    save_image(gt, os.path.join(\"./eva/gt\", f\"{i}.png\"))\n",
    "                    pred_image = net(input_image)\n",
    "                    save_image(pred_image, os.path.join(\"./eva/output\", f\"{i}.png\"))\n",
    "                    i += 1\n",
    "                test_time = time.time() - start_time\n",
    "                per_time = test_time / i\n",
    "                print(f\"test speed: {per_time} per image\")\n",
    "                results_path = \"./eva/output\"\n",
    "                gt_path = \"./eva/gt\"\n",
    "                imgsName = sorted(os.listdir(results_path))\n",
    "                gtsName = sorted(os.listdir(gt_path))\n",
    "                assert len(imgsName) == len(gtsName)\n",
    "    \n",
    "                cumulative_psnr, cumulative_ssim = 0, 0\n",
    "                for i in range(len(imgsName)):\n",
    "                    # logger('Processing image: %s' % (imgsName[i]))\n",
    "                    res = cv2.imread(os.path.join(results_path, imgsName[i]), cv2.IMREAD_COLOR)\n",
    "                    gt = cv2.imread(os.path.join(gt_path, gtsName[i]), cv2.IMREAD_COLOR)\n",
    "                    # logger(f\"image:{imgsName[i]}, gt:{gtsName[i]}\")\n",
    "                    cur_psnr = calculate_psnr(res, gt, test_y_channel=True)\n",
    "                    cur_ssim = calculate_ssim(res, gt, test_y_channel=True)\n",
    "                    # logger('PSNR is %.4f and SSIM is %.4f' % (cur_psnr, cur_ssim))\n",
    "                    cumulative_psnr += cur_psnr\n",
    "                    cumulative_ssim += cur_ssim\n",
    "                print('Testing set, PSNR is %.4f and SSIM is %.4f' % (\n",
    "                    cumulative_psnr / len(imgsName), cumulative_ssim / len(imgsName)))\n",
    "                psnr = cumulative_psnr / len(imgsName)\n",
    "                ssim = cumulative_ssim / len(imgsName)\n",
    "                if not os.path.exists(file_path):\n",
    "                    with open(file_path, 'w') as file:\n",
    "                        file.write(f\"steps:{total_steps}, PSNR:{psnr}, SSIM:{ssim}\\n\")\n",
    "                else:\n",
    "                    with open(file_path, 'a') as file:\n",
    "                        file.write(f\"steps:{total_steps}, PSNR:{psnr}, SSIM:{ssim}\\n\")\n",
    "                # --- Save the network parameters --- #\n",
    "                torch.save(net.state_dict(), './{}/{}_ckpt'.format(exp_name, total_steps))\n",
    "            torch.cuda.empty_cache()\n",
    "        if total_steps == num_steps:\n",
    "            print(\"Finish!\")\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33995b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
